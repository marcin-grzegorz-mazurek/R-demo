---
title: "Decision Trees and Random Forests"
author: "Marcin Mazurek"
date:  '2017-11-08'
output:
  html_document:
    toc: yes
  html_notebook:
    toc: yes
  pdf_document:
    toc: yes
---





#Classification trees 
##Przykładowy zbiór danych

###Metadata
http://archive.ics.uci.edu/ml/datasets/statlog+(heart)

#### Attribute Information: 

-- 1. age 

-- 2. sex 

-- 3. chest pain type (4 values) 

-- 4. resting blood pressure 

-- 5. serum cholestoral in mg/dl 

-- 6. fasting blood sugar > 120 mg/dl 

-- 7. resting electrocardiographic results (values 0,1,2) 

-- 8. maximum heart rate achieved 

-- 9. exercise induced angina 

-- 10. oldpeak = ST depression induced by exercise relative to rest 

-- 11. the slope of the peak exercise ST segment 

-- 12. number of major vessels (0-3) colored by flourosopy 

-- 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 

#### Attributes types 


Real: 1,4,5,8,10,12 

Ordered:11, 

Binary: 2,6,9 

Nominal:7,3,13 


#### Variable to be predicted 
 
Absence (1) or presence (2) of heart disease 

### Import data 
```{r}

library(ggplot2)


heart<-read.csv("heart.dat", sep=' ')

colnames(heart)<-c('age',
  'sex' ,
  'chest_pain_type' ,
  'blood_pressure' ,
  'cholestor',
  'sugar',
  'resting_electrocardiographic_results' ,
  'heart_rate', 
  'angina' ,
  'oldpeak', 
  'slope' ,
  'vessles' ,
  'thal',
  'disease')


head(heart)

#binary columns
bin_cols <- colnames(heart)[c(2,6,9)]
nom_cols <- colnames(heart)[c(7,3,13)] 

#
heart[bin_cols]<-lapply(heart[bin_cols], factor)
heart[nom_cols]<-lapply(heart[nom_cols], factor)

#target_columns
target_col<-'disease'
heart[target_col]<-lapply(heart[target_col], factor)
#losowe pomieszanie kolejnosci rekordow 

heart.shuffled<-heart[sample(nrow(heart)),]


#indeksy rozgraniczajce partycje (rekordy sa juz pomieszane)
idx1<-round(0.6*nrow(heart.shuffled))
idx2<-round(0.8*nrow(heart.shuffled))


train<-heart.shuffled[1:idx1,]
validation<-heart.shuffled[(idx1+1):idx2,]
test<-heart.shuffled[(idx2+1):nrow(heart.shuffled),]

predictors_string<-paste( c(bin_cols ,  nom_cols), collapse='+')
formula<-as.formula(paste('disease~', predictors_string))

###prior probabilities 

nrow(train)
t<-table(heart$disease)
prop.table(t)
```

##Budowa drzewa klasyfikującego

###Biblioteka tree 

####Budowa drzewa 
```{r}
library(tree)

heart.tree <- tree(formula, train)
summary(heart.tree)
heart.tree

plot(heart.tree)
text(heart.tree, pretty=0)


nodes <- heart.tree$frame[[1]]
print (nodes)

#liczba liści drzewa wynikowego 
nleaves<- length(nodes[nodes=='<leaf>'])

```

Liczba liści drzewa:  `r nleaves`

####Błąd klasyfikacji  
* ciąg trenujący

```{r}
p_disease<-predict(heart.tree, train )
number_misclassified <-  sum(max.col(p_disease)!=train$disease)
accuracy<-mean(max.col(p_disease)==train$disease)
misclasification_rate<-1-accuracy

```
 Liczba wszystkich obserwacji :  `r nrow(train)`

 Liczba błędnie zaklasyfikowanych: `r number_misclassified`

  Dokładność: `r accuracy`




* ciąg testowy 
```{r}
p_disease<-predict(heart.tree, test )
#Stosunek poprawnie zaklsyfikowanych do wszystkich 
number_misclassified <-  sum(max.col(p_disease)!=test$disease)
accuracy<-mean(max.col(p_disease)==test$disease)
misclasification_rate<-1-accuracy



```
 Liczba wszystkich obserwacji :  `r nrow(test)`

 Liczba błędnie zaklasyfikowanych: `r number_misclassified`

 Accuracy: `r accuracy`

```{r}

```

####Przycinanie (prunning)

Algorytm oparty na  kryterium kosztu - złożoności 


$Q(T)$ - miara błędu drzewa, na przykład ułamek błędnych klasyfikacji. 
Dla każdego nieujemnego współczynnika złożoności $\alpha$   szukamy takiego drzewa zakorzenionego w drzewie pełnym $T_{0}$. że wartość minimalną osiąga funkcja kosztu- złożoności wyrażona wzorem: 

$S_{\alpha}(T) = Q(T) + \alpha * |T|$

Przycinanie drzewa pełnego  $T_{0}$ przebiega w etapach: 
1.  Skonstruowanie ciągu poddrzew minimalizujących funkcję kosztu złożoności. J-te drzewo buduje się na podstawie dowolnie ustalonej  wartości $\alpha'_{j}$, wybieramy najmniejsze drzewo i oznaczamy je jako $T_{j}$
Wszystkie możliwe wartośći $\alpha$ można podzielić na rozłączne przedziały, wewnątrz których dla wszystkich $\alpha$ wybierane jest to samo drzewo. 

$T(\alpha) = T_{0}$  dla  $\alpha < \alpha_{1}$

$T(\alpha) = T_{k}$  dla  $\alpha_{k} \le \alpha \le \alpha_{k+1}$, $1 \le k \le K$

$T(\alpha) = T_{K}$  dla  $\alpha \ge \alpha_{K}$

gdzie $K$ jest najmniejszym indeksem, dla którego wybrany został sam korzeń. 


2. Wybranie z ciągu $T_{1}, T_{2}, \dots, T_{K}$ drzewa  o najmniejszym ułamku błędów klasyfikacji dla próby testowej, lub w przypadku gdy nie dysponujemy próbą testową wybór najlepszego drzewa odbywa się na podstawie kroswalidacji.


Sekwencja drzew budowana w z wykorzystaniem  liczby liści:  

```{r}
for (leaves_cnt in nleaves:1 ){
  print(leaves_cnt)
  pruned.heart.tree = prune.misclass(heart.tree, best=leaves_cnt)
  print(summary(pruned.heart.tree))
}
```


Sekwencja drzew budowana w z wykorzystaniem  liczby liści: 

```{r}
for (alfa in seq(50,-10,-5)){
  print(alfa)
  pruned.heart.tree = prune.misclass(heart.tree, k=alfa)
  print(summary(pruned.heart.tree))
}

```

```{r}
  pruned.heart.tree = prune.misclass(heart.tree, best=1)
  print(summary(pruned.heart.tree))


  ##plot(pruned.heart.tree)
  #text(pruned.heart.tree, pretty=0)
  

```


Wyznaczenie sekwencji drzew - w wyniku otrzymujemy liczbę liści drzewa, odpowiadający jej bład klasyfikacji oraz  graniczną wartość parametru związanego z kosztem związanym z rozmiarem drzewa. 

```{r}

#przycinanie drzewa: wykreślenie błędu w zależności od liczby liśći 
#FUN - kryterium przycianania, 
cv.heart.tree<-cv.tree(heart.tree, FUN = prune.misclass)
cv.heart.tree

```


Wykres przedstawiający błąd klasyfikacji w zależności od liczby liści oraz wartości parametru: 

```{r}
par(mfrow=c(1,2))
plot(cv.heart.tree$size, cv.heart.tree$dev, type="b")
plot(cv.heart.tree$k, cv.heart.tree$dev, type="b")

```



Uzyskana sekwencja drzew: 
```{r  fig.width=8, fig.height=4, warning=FALSE, eval=TRUE, message=FALSE, tidy=TRUE, fig.show='hold', fig.align='center'}
for (leaves_cnt in cv.heart.tree$size[1:length(cv.heart.tree$size)-1] ){
  
  pruned.heart.tree=prune.misclass(heart.tree, best=leaves_cnt)
  print(summary(pruned.heart.tree))
  plot(pruned.heart.tree)
  text(pruned.heart.tree, pretty=0)

  
}
```
```{r}
plot.tree.sequence(cv.heart.tree)
```

```{r}

pruned.heart.tree=prune.misclass(heart.tree, best=4)
summary(pruned.heart.tree)


plot(pruned.heart.tree)
text(pruned.heart.tree, pretty=0)


deviance(heart.tree)
misclass.tree(heart.tree, detail=FALSE)
#Liczba błędnych klasyfikacji w każdym z węzłów (niekoniecznie liści)
misclass.tree(heart.tree, detail=TRUE)

```
####Błąd klasyfikacji 

```{r}
p_disease<-predict(pruned.heart.tree, test )
#Stosunek poprawnie zaklsyfikowanych do wszystkich 
accuracy<-mean(max.col(p_disease)==test$disease)
misclasification_rate<-1-accuracy

accuracy

```


####Biblioteka rpart 

```{r}

library(rpart)
library(rpart.plot)


heart.tree <- rpart(formula, train ,method="class")
summary(heart.tree)
rpart.plot(heart.tree, uniform=TRUE)

```



```{r}

printcp(heart.tree)
```

Jak wyznaczyć miejsce przycięcia:



Wybrać drzewo o najmniejszym błędzie na zbiorze walidującym (xerror)


```{r}




opt <- which.min(heart.tree$cptable[,'xerror'])
cp<-heart.tree$cptable[opt,"CP"]


```
Najlepszy podział : `r opt`

Wartość complexity-cost (cp) : `r cp`
```{r}

pruned<-prune.rpart(heart.tree,cp)
rpart.plot(pruned, uniform=TRUE)


```
Inne podejście: 

As a rule of thumb, it’s best to prune a decision tree using the cp of smallest tree that is within one standard deviation of the tree with the smallest xerror. 


##Random forest
```{r} 
library (randomForest)


rforest<-randomForest(formula, train, ntree=50)

p_disease<-predict(rforest, test )
#Stosunek poprawnie zaklsyfikowanych do wszystkich 
accuracy<-mean(p_disease==test$disease)
misclasification_rate<-1-accuracy

accuracy

```

#Regression trees
